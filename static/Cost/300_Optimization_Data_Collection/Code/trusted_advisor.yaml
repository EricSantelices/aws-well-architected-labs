AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves Inventory data for the chosen service
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: Trusted_Advisor
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  Prefix:
    Type: String
    Description: Service which the data collector is looking at
    Default: ta
  RolePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  GlueRoleARN:
    Type: String
Outputs:
  LambdaRoleARN:
    Description: Role for Lambda execution of lambda data.
    Value:
      Fn::GetAtt:
        - LambdaRole
        - Arn
  GlueCrawler:
    Value:
      Fn::Sub: "${CFDataName}Crawler"
  SQSUrl:
    Description: TaskQueue URL the account collector lambda
    Value: !Ref TaskQueue
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${RolePrefix}Lambda-Role-${CFDataName}"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        - PolicyName: !Sub "Assume-Management-${CFDataName}-Account-Role"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: "*"
        - PolicyName: !Sub "${CFDataName}-S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                Resource:
                  !Ref DestinationBucketARN
              - Effect: "Allow"
                Action:
                  - "glue:StartCrawler"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "sqs:ReceiveMessage"
                  - "sqs:DeleteMessage"
                  - "sqs:GetQueueAttributes"
                Resource: "*"
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${CFDataName}-Lambda-Function"
      Description: !Sub "LambdaFunction to retrieve ${CFDataName}"
      Runtime: python3.8
      Code:
        ZipFile: |
          import os
          import json
          from datetime import date, datetime

          import boto3
          from botocore.exceptions import ClientError
          from botocore.client import Config

          prefix = os.environ["PREFIX"]
          bucket = os.environ["BUCKET_NAME"]
          only_cost = os.environ['COSTONLY']
          crawler = os.environ["CRAWLER_NAME"]

          def lambda_handler(event, context):
              try:
                  for record in event['Records']:
                      body = json.loads(record["body"])
                      acc_id = body["account_id"]
                      acc_name = body["account_name"]
                      if prefix != 'ta':
                          print(f"These aren't the datapoints you're looking for: {prefix}")
                          continue
                      main(acc_id, acc_name)
                      upload_to_s3(prefix, acc_id)
                      start_crawler()
              except Exception as e:
                  print(f'ERROR: {e}')

          def upload_to_s3(prefix, acc_id):
              if not os.path.getsize("/tmp/data.json"):
                  print(f"No data in file for {prefix}")
                  return
              date_formatted = datetime.now().strftime("%d%m%Y-%H%M%S")
              today = date.today()
              path = f"optics-data-collector/{prefix}-data/year={today.year}/month={today.month}/{prefix}-{acc_id}-{date_formatted}.json"
              try:
                  s3 = boto3.client("s3", config=Config(s3={"addressing_style": "path"}))
                  s3.upload_file("/tmp/data.json", bucket, path)
                  print(f"Data {acc_id} in s3 - {bucket}/{path}")
              except Exception as e:
                  print(f'ERROR: upload - {e}')

          def assume_role(acc_id, service, region):
              role_name = os.environ['ROLENAME']
              role_arn = f"arn:aws:iam::{acc_id}:role/{role_name}" #OrganizationAccountAccessRole
              sts_client = boto3.client('sts')
              try:
                  #region = sts_client.meta.region_name
                  assumed = sts_client.assume_role(
                      RoleArn=role_arn,
                      RoleSessionName="AssumeRoleRoot"
                  )
                  creds = assumed['Credentials']
                  return boto3.client(
                      service,
                      aws_access_key_id=creds['AccessKeyId'],
                      aws_secret_access_key=creds['SecretAccessKey'],
                      aws_session_token=creds['SessionToken'],
                      region_name=region
                  )
              except ClientError as e:
                  print(f"Unexpected error Account {acc_id}: {e}")

          def start_crawler():
              try:
                  boto3.client("glue").start_crawler(Name=crawler)
              except Exception as e:
                  print(f"ERROR: crawler start - {e}")

          class DateTimeEncoder(json.JSONEncoder):
              def default(self, obj):
                  if isinstance(obj, (date, datetime)):
                      return obj.isoformat()

          def epoc(ts):
              return int((datetime.strptime(ts, '%Y-%m-%dT%H:%M:%SZ') - datetime(1970, 1, 1)).total_seconds())

          def main(acc_id, account_name):
              f = open("/tmp/data.json", "w")  # Saving in the temporay folder in the lambda
              support = assume_role(acc_id, "support", "us-east-1")
              resp = support.describe_trusted_advisor_checks(language="en")
              for check in resp["checks"]:
                  base = {"AccountId":acc_id, "Category": check["category"], "AccountName":account_name}
                  try:
                      meta = check["metadata"]
                      if only_cost == 'yes' and check["category"] != 'cost_optimizing': continue
                      id = check["id"]
                      name = {"CheckName": check["name"], "CheckId": id}
                      result = support.describe_trusted_advisor_check_result(checkId=id, language="en")['result']
                      if result.get('status') == 'not_available':
                          continue
                      ts = result['timestamp']
                      base.update({'DateTime':ts, 'Timestamp':epoc(ts)})
                      for rsrc in result["flaggedResources"]:
                          res = {}
                          if "metadata" in rsrc:
                              res = dict(zip(meta, rsrc["metadata"]))
                              del rsrc['metadata']
                          rsrc["Region"] = rsrc.pop("region", '-')
                          rsrc["Status"] = rsrc.pop("status", '-')
                          res.update(base)
                          res.update(name)
                          res.update(rsrc)
                          f.write(json.dumps(res, cls=DateTimeEncoder) + "\n")
                  except Exception as e:
                      print(f"ERROR: {check.get('name')} - {e}")
                      continue
              f.close()
      Handler: 'main.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Environment:
        Variables:
          BUCKET_NAME:
            Ref: DestinationBucket
          ROLENAME:
            Ref: MultiAccountRoleName
          PATH:
            Ref: CFDataName
          CRAWLER_NAME:
            Ref: Crawler
          PREFIX:
            Ref: Prefix
          COSTONLY:
            "no"
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name:
        !Sub "${CFDataName}Crawler"
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/optics-data-collector/${Prefix}-data/"
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaFunction.Arn
      Action: lambda:InvokeFunction
      Principal: sqs.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt TaskQueue.Arn
  TaskQueue: 
    Type: AWS::SQS::Queue
    Properties: 
      VisibilityTimeout: 300
      ReceiveMessageWaitTimeSeconds: 20
      DelaySeconds: 2
      KmsMasterKeyId: "alias/aws/sqs"
  EventSourceMapping:
    DependsOn:
      - EventPermission
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt TaskQueue.Arn
      FunctionName: !GetAtt LambdaFunction.Arn